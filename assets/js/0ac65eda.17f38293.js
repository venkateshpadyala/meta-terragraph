"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9929],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>h});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(t),h=i,m=u["".concat(s,".").concat(h)]||u[h]||c[h]||r;return t?a.createElement(m,o(o({ref:n},d),{},{components:t})):a.createElement(m,o({ref:n},d))}));function h(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var p=2;p<r;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},5765:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=t(7462),i=(t(7294),t(3905));const r={},o="Routing Layer",l={unversionedId:"developer/Routing_Layer",id:"developer/Routing_Layer",title:"Routing Layer",description:"This document gives a high-level description of Terragraph's routing layer.",source:"@site/../docs/developer/Routing_Layer.md",sourceDirName:"developer",slug:"/developer/Routing_Layer",permalink:"/docs/developer/Routing_Layer",draft:!1,editUrl:"https://github.com/terragraph/meta-terragraph/edit/main/docs/../docs/developer/Routing_Layer.md",tags:[],version:"current",frontMatter:{},sidebar:"developerManualSidebar",previous:{title:"Communication Protocol",permalink:"/docs/developer/Communication_Protocol"},next:{title:"Driver Interface",permalink:"/docs/developer/Driver_Interface"}},s={},p=[{value:"Network Topology",id:"network-topology",level:2},{value:"Routing",id:"routing",level:2},{value:"About Open/R",id:"about-openr",level:3},{value:"Open/R CLI",id:"openr-cli",level:3},{value:"Terragraph Routing",id:"terragraph-routing",level:3},{value:"Open/R on WiGig and Wired interfaces",id:"openr-on-wigig-and-wired-interfaces",level:3},{value:"Open/R Usage In E2E",id:"openr-usage-in-e2e",level:2},{value:"Node Configuration",id:"node-configuration",level:3},{value:"Prefix Allocation",id:"prefix-allocation",level:3},{value:"MCS-Based Routing",id:"mcs-based-routing",level:3},{value:"Soft Draining",id:"soft-draining",level:3},{value:"Fixed Link Metrics",id:"fixed-link-metrics",level:3},{value:"Routing Queries",id:"routing-queries",level:3},{value:"BGP Implementation",id:"bgp-implementation",level:2},{value:"ExaBGP",id:"exabgp",level:3},{value:"FRRouting",id:"frrouting",level:3},{value:"frr-reload",id:"frr-reload",level:4},{value:"Routing for IPv4",id:"routing-for-ipv4",level:2},{value:"L2 Tunneling",id:"l2-tunneling",level:3},{value:"SRv6",id:"srv6",level:4},{value:"VXLAN",id:"vxlan",level:4},{value:"L2 Tunnel Configuration",id:"l2-tunnel-configuration",level:4},{value:"NAT64",id:"nat64",level:3},{value:"Policing and Classification",id:"policing-and-classification",level:2},{value:"HQoS Configuration",id:"hqos-configuration",level:2},{value:"vppctl",id:"vppctl",level:3},{value:"vpp_chaperone",id:"vpp_chaperone",level:3},{value:"Weighted Round Robin Scheduling",id:"weighted-round-robin-scheduling",level:3},{value:"CPE Configuration",id:"cpe-configuration",level:2},{value:"Resources",id:"resources",level:2}],d={toc:p};function c(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"routing-layer"},"Routing Layer"),(0,i.kt)("p",null,"This document gives a high-level description of Terragraph's routing layer."),(0,i.kt)("h2",{id:"network-topology"},"Network Topology"),(0,i.kt)("p",null,'Terragraph is a wireless mesh network comprised of "distribution nodes" (DN) and\n"client nodes" (CN). One or more DNs must have fiber connectivity to the\nInternet, and are designated as a "points of presence" (POP). The remaining DNs\nare responsible for transferring traffic over multiple hops to and from the CNs.\nBoth DNs and CNs are connection points for wireless access points (APs) and\nother customer premise equipment (CPE) to connect to the Internet.'),(0,i.kt)("p",{align:"center"},(0,i.kt)("img",{src:"/figures/deployment.png",width:"1000"})),(0,i.kt)("h2",{id:"routing"},"Routing"),(0,i.kt)("h3",{id:"about-openr"},"About Open/R"),(0,i.kt)("p",null,"Terragraph uses ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebook/openr"},"Open/R")," as its routing platform. Open/R is comprised of several\ndistinct modules, and uses ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/facebook/fbthrift"},"fbthrift")," for serialization and transit. The core\nmodule in Open/R is the distributed, eventually-consistent key-value store,\nnamed ",(0,i.kt)("inlineCode",{parentName:"p"},"KvStore"),", which is used to disseminate information such as routing\nadjacencies and network prefixes across the entire network; this is used to\nimplement a link-state routing protocol."),(0,i.kt)("p",null,'In short, Open/R computes routes by building a graph using the adjacency and\nprefix information in the key-value store. It runs a weighted shortest-paths\nalgorithm to all other nodes, and uses equal-cost multi-path routing (ECMP) when\nmultiple "best paths" exist. The best next-hops are then programmed into\nhardware.'),(0,i.kt)("p",null,"Open/R's modules are listed below."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Module"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"KvStore")),(0,i.kt)("td",{parentName:"tr",align:null},"Distributed key-value store")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"LinkMonitor")),(0,i.kt)("td",{parentName:"tr",align:null},"Link discovery and monitoring module")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"Spark")),(0,i.kt)("td",{parentName:"tr",align:null},"Neighbor discovery module")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"Decision")),(0,i.kt)("td",{parentName:"tr",align:null},"Route computation module")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"Fib")),(0,i.kt)("td",{parentName:"tr",align:null},"Route programming module")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"PrefixManager")),(0,i.kt)("td",{parentName:"tr",align:null},"Prefix management module")))),(0,i.kt)("h3",{id:"openr-cli"},"Open/R CLI"),(0,i.kt)("p",null,"Open/R provides a Python command-line interface, ",(0,i.kt)("inlineCode",{parentName:"p"},"breeze"),", to interact with each\nof its modules over Thrift."),(0,i.kt)("p",null,"In Terragraph, ",(0,i.kt)("inlineCode",{parentName:"p"},"breeze")," is replaced with a lightweight Lua version called\n",(0,i.kt)("inlineCode",{parentName:"p"},"puff"),". Usage is largely the same, and ",(0,i.kt)("inlineCode",{parentName:"p"},"puff")," implements most of the original\nCLI's commands. ",(0,i.kt)("inlineCode",{parentName:"p"},"puff")," source code resides in ",(0,i.kt)("inlineCode",{parentName:"p"},"src/terragraph-e2e/lua/puff.lua"),",\nand unit tests are located in ",(0,i.kt)("inlineCode",{parentName:"p"},"src/terragraph-e2e/lua/tests/puff_test.lua"),"."),(0,i.kt)("h3",{id:"terragraph-routing"},"Terragraph Routing"),(0,i.kt)("p",null,"Each Terragraph node runs an ",(0,i.kt)("inlineCode",{parentName:"p"},"openr")," process. On Puma hardware, nodes run an\nadditional ",(0,i.kt)("inlineCode",{parentName:"p"},"fib_vpp")," process to program routes into ",(0,i.kt)("a",{parentName:"p",href:"https://wiki.fd.io/view/VPP"},"VPP"),"'s FIB."),(0,i.kt)("p",null,"To route Internet traffic, all POP sectors advertise a default prefix ",(0,i.kt)("inlineCode",{parentName:"p"},"::/0"),",\nand other sectors forward default traffic to the nearest POP. At the POP sector,\negress traffic is handled by either adding ",(0,i.kt)("em",{parentName:"p"},"static routes")," in Terragraph's ",(0,i.kt)("em",{parentName:"p"},"fib\nimplementation")," or through ",(0,i.kt)("em",{parentName:"p"},"BGP peering"),". These POP settings are configured in\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"popParams")," structure in the node configuration."),(0,i.kt)("p",null,"Note that Open/R names nodes by their MAC addresses, but with a different\nrepresentation from E2E. For example, an E2E MAC address ",(0,i.kt)("inlineCode",{parentName:"p"},"00:00:00:10:0d:40")," is\nrepresented in Open/R as ",(0,i.kt)("inlineCode",{parentName:"p"},"node-00.00.00.10.0d.40"),"."),(0,i.kt)("p",null,'For CNs, Open/R supports running in "leaf" mode via the ',(0,i.kt)("inlineCode",{parentName:"p"},"set_leaf_node")," flag.\nLeaf nodes will only track a minimal set of keys in ",(0,i.kt)("inlineCode",{parentName:"p"},"KvStore"),", reducing Open/R's\nmemory footprint. This mode of operation is not in use on Puma hardware."),(0,i.kt)("h3",{id:"openr-on-wigig-and-wired-interfaces"},"Open/R on WiGig and Wired interfaces"),(0,i.kt)("p",null,"Open/R by default is enabled on the WiGig ",(0,i.kt)("inlineCode",{parentName:"p"},"terraX")," interfaces in the node\nconfiguration using ",(0,i.kt)("inlineCode",{parentName:"p"},"OPENR_IFACE_REGEX_INCLUDE"),'. Since Open/R can only listen\nand discover peers on the Linux interfaces, Linux tap interfaces are set up on\nbehalf of its VPP counterparts to allow forwarding of local or "slowpath"\ntraffic (in Puma VPP/Linux split architecture). Open/R discovers WiGig peers\nover ',(0,i.kt)("inlineCode",{parentName:"p"},"terraX")," while the actual VPP routing and FIB route programming is through\nthe corresponding ",(0,i.kt)("inlineCode",{parentName:"p"},"vpp-terraX")," interface."),(0,i.kt)("p",null,"For Open/R over wired POP interfaces, ",(0,i.kt)("inlineCode",{parentName:"p"},"tapX")," Linux interface should be added to\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"OPENR_IFACE_REGEX_INCLUDE")," regular expression and the routing would be\nthrough the ",(0,i.kt)("inlineCode",{parentName:"p"},"loopX")," L3 interface which functions as a Bridged VLAN interface.\nThis is enabled by default in Puma configuration."),(0,i.kt)("h2",{id:"openr-usage-in-e2e"},"Open/R Usage In E2E"),(0,i.kt)("p",null,"All communication with Open/R occurs through the local ",(0,i.kt)("inlineCode",{parentName:"p"},"openr")," process on a\nnode; the controller never connects to any node's Open/R sockets directly, but\ninstead issues commands through a minion's ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp"),"."),(0,i.kt)("h3",{id:"node-configuration"},"Node Configuration"),(0,i.kt)("p",null,"E2E depends on ",(0,i.kt)("inlineCode",{parentName:"p"},"KvStore")," to distribute important configuration values that are\nneeded before nodes can reach the controller, such as the controller URL\n(",(0,i.kt)("inlineCode",{parentName:"p"},"e2e-ctrl-url"),") and network seed prefix (",(0,i.kt)("inlineCode",{parentName:"p"},"e2e-network-prefix"),"). These key-value\npairs are located in the ",(0,i.kt)("inlineCode",{parentName:"p"},"kvstoreParams")," map in the node configuration. Because\nthe key-value store is distributed, only one node needs to have this config set\n(usually a POP node). These keys get injected during node startup in the\n",(0,i.kt)("inlineCode",{parentName:"p"},"pop_config")," script, and ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp")," periodically polls for them and\noverwrites unexpected values."),(0,i.kt)("p",null,"To isolate access to Open/R, ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp")," writes and periodically syncs keys\nto a file ",(0,i.kt)("inlineCode",{parentName:"p"},"/tmp/mynetworkinfo"),", which contains a JSON-serialized ",(0,i.kt)("inlineCode",{parentName:"p"},"NetworkInfo"),"\nThrift structure. Fields include the controller URL (primary and backup),\naggregator URL(s), network seed prefix, and DHCP/BGP configuration. This file is\nread by the minion's ",(0,i.kt)("inlineCode",{parentName:"p"},"Broker"),", as well as the stats modules and Terragraph's\n",(0,i.kt)("inlineCode",{parentName:"p"},"squire_linux")," daemon."),(0,i.kt)("p",null,"The node configuration contains a number of settings for the Open/R program.\nBefore Open/R starts, a configuration template file\n(",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/sysconfig/openr_config_template.txt"),") is filled out using the node\nconfiguration and written to ",(0,i.kt)("inlineCode",{parentName:"p"},"/var/run/openr_config.json"),"."),(0,i.kt)("h3",{id:"prefix-allocation"},"Prefix Allocation"),(0,i.kt)("p",null,"The controller can be set up to centrally allocate node prefixes, in place of\nthe default distributed allocation procedure. If enabled, the controller\nconstructs a ",(0,i.kt)("inlineCode",{parentName:"p"},"StaticAllocation")," Thrift structure mapping each node name to its\nprefix. This gets sent to ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp"),", which then sets the ",(0,i.kt)("inlineCode",{parentName:"p"},"KvStore")," key\n",(0,i.kt)("inlineCode",{parentName:"p"},"e2e-network-allocations")," to this value. Open/R's ",(0,i.kt)("inlineCode",{parentName:"p"},"PrefixAllocator")," will use\nthis static allocation instead of the network seed prefix\n(",(0,i.kt)("inlineCode",{parentName:"p"},"e2e-network-prefix"),")."),(0,i.kt)("h3",{id:"mcs-based-routing"},"MCS-Based Routing"),(0,i.kt)("p",null,"E2E can be configured to automatically adjust Open/R link metrics (i.e. edge\nweights in the shortest-paths routing graph) based on real-time statistics at\neach node. Currently, the firmware reports the MCS (modulation and coding scheme\nindex) on each link to the minion's ",(0,i.kt)("inlineCode",{parentName:"p"},"StatusApp")," approximately every 100ms. The\nminion will then set the link metric via ",(0,i.kt)("inlineCode",{parentName:"p"},"LinkMonitor")," so that lower-quality\nlinks (i.e. with lower MCS) are used less frequently than better ones. Link\nmetric changes are rate-limited to avoid causing frequent shortest-paths\nre-computations across the network, using a combination of two methods. First is\na dampening algorithm, which ignores reported MCS values until seeing ",(0,i.kt)("em",{parentName:"p"},"N"),"\nconsecutive values that would either all increase or all decrease the current\nlink metric. The second is a token bucket, which fills at a rate ",(0,i.kt)("em",{parentName:"p"},"r")," with a\nmaximum burst size ",(0,i.kt)("em",{parentName:"p"},"b"),". Link metric changes are only allowed when a token is\navailable. Relevant options are located in the ",(0,i.kt)("inlineCode",{parentName:"p"},"openrParams.linkMetricConfig"),"\nstructure in the node configuration. All link metrics are effectively 1 by\ndefault. ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp")," periodically polls link metric information from\n",(0,i.kt)("inlineCode",{parentName:"p"},"LinkMonitor")," and overwrites any unexpected values."),(0,i.kt)("h3",{id:"soft-draining"},"Soft Draining"),(0,i.kt)("p",null,'Individual links can be "soft drained" via the per-link parameters in the node\nconfiguration, located at\n',(0,i.kt)("inlineCode",{parentName:"p"},"linkParamsOverride.<macAddr>.openrLinkParams.softDisable"),". A soft-drained link\nsimply has a very high link metric (100000), and will be avoided unless no other\npaths are possible. This configuration is handled by ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenrClientApp"),", which\nsets the link metric in ",(0,i.kt)("inlineCode",{parentName:"p"},"LinkMonitor")," in the same procedure as described above."),(0,i.kt)("h3",{id:"fixed-link-metrics"},"Fixed Link Metrics"),(0,i.kt)("p",null,"Similar to soft draining, links can have fixed metrics assigned to them via the\nsame procedure. This setting is defined in the node configuration at\n",(0,i.kt)("inlineCode",{parentName:"p"},"linkParamsOverride.<macAddr>.openrLinkParams.fixedMetric"),"."),(0,i.kt)("h3",{id:"routing-queries"},"Routing Queries"),(0,i.kt)("p",null,"The controller exposes an API to fetch all shortest routes between a source and\ndestination node. ",(0,i.kt)("inlineCode",{parentName:"p"},"TopologyApp")," periodically queries a node for a dump of\nadjacencies (",(0,i.kt)("inlineCode",{parentName:"p"},"AdjacencyDatabase"),") and prefixes (",(0,i.kt)("inlineCode",{parentName:"p"},"PrefixDatabase"),") in ",(0,i.kt)("inlineCode",{parentName:"p"},"KvStore"),"\nfor the entire network. To answer route queries, ",(0,i.kt)("inlineCode",{parentName:"p"},"TopologyApp")," then runs the\n",(0,i.kt)("inlineCode",{parentName:"p"},"Fib")," shortest paths solver recursively from the source to destination node,\nrecording the best-hop at each step."),(0,i.kt)("h2",{id:"bgp-implementation"},"BGP Implementation"),(0,i.kt)("p",null,"Terragraph supports multiple ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Border_Gateway_Protocol"},"BGP")," daemons. Logic for managing BGP applications\nis captured in a BGP wrapper script (",(0,i.kt)("inlineCode",{parentName:"p"},"/usr/sbin/bgp_wrapper.sh"),"). Currently,\nTerragraph uses ",(0,i.kt)("a",{parentName:"p",href:"https://pypi.org/project/exabgp/"},"ExaBGP")," by default, but will fall back on ",(0,i.kt)("a",{parentName:"p",href:"https://frrouting.org/"},"FRRouting")," if ExaBGP\nis not installed. E2E monitors BGP status on PoP nodes via calls to the\nrespective CLIs in the ",(0,i.kt)("inlineCode",{parentName:"p"},"BgpUtils")," class."),(0,i.kt)("p",null,"A custom init.d script is installed to gracefully shut down BGP connections\nbefore reboot (",(0,i.kt)("inlineCode",{parentName:"p"},"/etc/init.d/bgp_softshut"),"). This will stop the BGP daemon and\nsend a BGP Cease Notification message to peers, withdraw the default route from\nOpen/R, and briefly wait while transit traffic drains from links."),(0,i.kt)("p",null,"This section will describe Terragraph's BGP implementations for VPP-based\nplatforms only."),(0,i.kt)("h3",{id:"exabgp"},"ExaBGP"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://pypi.org/project/exabgp/"},"ExaBGP")," is a Python-based BGP application offering easy programmatic control.\n",(0,i.kt)("inlineCode",{parentName:"p"},"exabgpcli")," exists to check ExaBGP state, operating on named pipes\n(",(0,i.kt)("inlineCode",{parentName:"p"},"/run/exabgp/exabgp.in"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"/run/exabgp/exabgp.out"),")."),(0,i.kt)("p",null,"ExaBGP forks a plugin named ",(0,i.kt)("inlineCode",{parentName:"p"},"exabgp-to-openr")," which parses BGP messages in JSON\nformat, redistributes learned prefixes, and produces JSON to advertise prefixes.\nThe plugin exports and health-checks routes via a ",(0,i.kt)("a",{parentName:"p",href:"https://thrift.apache.org/"},"Thrift")," API to ",(0,i.kt)("inlineCode",{parentName:"p"},"fib_vpp")," and\nOpen/R. ",(0,i.kt)("inlineCode",{parentName:"p"},"fib_vpp")," installs the best routes into VPP's ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Forwarding_information_base"},"FIB"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"exabgp-to-openr"),"\nalso performs health checks before the PoP node will advertise a prefix to its\nBGP peers via ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenRChecker"),", which ensures that the PoP node has an allocated\nprefix (on Linux's ",(0,i.kt)("inlineCode",{parentName:"p"},"lo")," interface) ",(0,i.kt)("em",{parentName:"p"},"or")," the Open/R RIB has a subnet in the\ndesired prefix to advertise. Lastly, the plugin dynamically advertises/withdraws\nCPE prefixes found in Open/R to BGP peers."),(0,i.kt)("p",null,"As ExaBGP starts up, a configuration generation process (",(0,i.kt)("inlineCode",{parentName:"p"},"exaconf"),") is run to\npull information from node configuration (",(0,i.kt)("inlineCode",{parentName:"p"},"popParams")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"bgpParams"),") and\ngenerate the following files:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"/data/etc/exabgp/aioexabgp.conf"),": JSON configuration controlling what\nprefixes to advertise and learn"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"/data/etc/exabgp/exabgp.conf"),": Standard ExaBGP configuration to control who\nto peer with")),(0,i.kt)("p",null,"Useful log files are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"/var/log/exabgp/current"),": ExaBGP configuration generation and operation logs"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"/var/log/pop_config/current"),": Output from configuring the node to be a PoP")),(0,i.kt)("h3",{id:"frrouting"},"FRRouting"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://frrouting.org/"},"FRRouting")," (or FRR) is a C-based suite of network routing protocols originally\nforked from Quagga. It provides a long list of protocol daemons (such as ",(0,i.kt)("inlineCode",{parentName:"p"},"bgpd"),")\nwhich are centrally managed by a ",(0,i.kt)("inlineCode",{parentName:"p"},"zebra")," process to handle communications with\nthe data plane. A unified shell, ",(0,i.kt)("inlineCode",{parentName:"p"},"vtysh"),", exists to interface with all daemons."),(0,i.kt)("p",null,"The BGP wrapper script starts all services used by FRR:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"bgpd")," - The FRR BGP daemon. The startup script generates the initial\nconfiguration file, ",(0,i.kt)("inlineCode",{parentName:"li"},"/var/run/bgpd.conf"),", from the node configuration via\n",(0,i.kt)("inlineCode",{parentName:"li"},"src/terragraph-e2e/lua/update_bgpd_conf.lua"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"zebra"),' - The FRR Zebra daemon. Zebra exposes a "FIB push" interface via a\nForwarding Plane Manager (FPM) module, which allows an external process to\nconnect and receive a complete copy of the forwarding tables over Netlink\nmessages.'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"frr_openr_sync")," - A Lua service which connects to Zebra's FPM socket and\nsyncs BGP routes with the ",(0,i.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Forwarding_information_base"},"FIB")," (add/delete via ",(0,i.kt)("a",{parentName:"li",href:"https://thrift.apache.org/"},"Thrift")," API to ",(0,i.kt)("inlineCode",{parentName:"li"},"Fib")," agent)\nand Open/R (advertise/withdraw via ",(0,i.kt)("inlineCode",{parentName:"li"},"breeze")," CLI)."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"frr_bgp_healthcheck")," - A Lua service which health-checks and manages the\nannouncement of routes to ",(0,i.kt)("inlineCode",{parentName:"li"},"bgpd"),". The service monitors all Terragraph prefixes\nfrom the node configuration to ensure reachability before advertising them to\nBGP peers (i.e. PoP loopback prefix is within the desired prefix, ",(0,i.kt)("em",{parentName:"li"},"or")," the\nOpen/R RIB has a subnet in the desired prefix), and also monitors all CPE\nprefixes found in Open/R. Any changes to advertised BGP routes are dynamically\napplied to the FRR configuration via the ",(0,i.kt)("inlineCode",{parentName:"li"},"frr-reload")," utility (see below).")),(0,i.kt)("h4",{id:"frr-reload"},"frr-reload"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"frr-reload")," utility (",(0,i.kt)("inlineCode",{parentName:"p"},"src/terragraph-e2e/lua/frr_reload.lua"),") is a Lua port\nof the ",(0,i.kt)("inlineCode",{parentName:"p"},"frr-reload.py")," Python script distributed with FRR. This utility compares\ntwo FRR configurations (e.g. the running configuration against a desired\nconfiguration file) and runs a sequence of ",(0,i.kt)("inlineCode",{parentName:"p"},"vtysh")," commands to achieve the\ntarget configuration without restarting any daemons. Only relevant portions of\nthe script, relating to BGP features discussed above, have been ported, and\nother unused configuration sections are not implemented."),(0,i.kt)("p",null,"The script uses two internal classes to model FRR configuration:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"Context")," - Represents a section of the configuration, where a ",(0,i.kt)("em",{parentName:"li"},"context")," is\nloosely defined as a series of lines with a unique configuration meaning when\ngrouped together. Some single-line contexts exist as well."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"Config")," - Groups together all unique contexts of the configuration.")),(0,i.kt)("p",null,"Contexts are parsed in ",(0,i.kt)("inlineCode",{parentName:"p"},"Config:loadContexts()")," and compared in\n",(0,i.kt)("inlineCode",{parentName:"p"},"FrrReloadUtils.compareContextObjects()"),". Applying the FRR configuration changes\nrequires two passes, where deletions of lines occurs only in the first pass."),(0,i.kt)("h2",{id:"routing-for-ipv4"},"Routing for IPv4"),(0,i.kt)("p",null,"Terragraph is a native IPv6 network. To enable IPv4-only Terragraph deployments,\nwhere both access networks and/or ISP networks can be IPv4, a combination of L2\ntunneling and NAT64 is recommended. L2 tunnels will be set up for all data-plane\ntraffic between POP nodes and CNs encapsulating IPv4 traffic with the\nappropriate tunnel headers. NAT64 will handle IPv4 control-plane traffic to\nTerragraph cloud services (e.g. E2E, NMS, Kafka, Fluentd)."),(0,i.kt)("h3",{id:"l2-tunneling"},"L2 Tunneling"),(0,i.kt)("p",null,"Terragraph supports SRv6 and VXLAN tunneling options for data-plane traffic\nbetween the POP nodes and CNs (currently only in VPP mode)."),(0,i.kt)("h4",{id:"srv6"},"SRv6"),(0,i.kt)("p",null,'SRv6 (Segment Routing) is the recommended solution for L2 tunneling, as it\nprovides a modular framework for L2 tunneling with SRv6 "segments" using the\nnative IPv6 header. An SRv6 L2 tunnel can be formed between a POP DN and a CN by\nencapsulating an IPv6 header and SRv6 segment headers over an L2 frame carrying\nIPv4 traffic.'),(0,i.kt)("p",null,"To create an SRv6 L2 tunnel originating from a POP node and terminating at a CN,\nan SRv6 encapsulation policy is defined on the POP interface\n(",(0,i.kt)("inlineCode",{parentName:"p"},"popParams.POP_IFACE"),") or it's VLAN sub-interface, which adds IPv6 + SRv6\nheaders on all L2 frames received on that interface. The destination for the\nencapsulated SRv6 packets is an SRv6 decapsulation policy on the CN's physical\nCPE interfaces. The CN's decapsulation policy then removes the IPv6 + SRv6\nheaders and send the L2 frames out to the CPE interface. Thus an SRv6 L2 tunnel\nin the Terragraph network is a single SRv6 segment from one node to another. The\nreverse tunnel from CN to POP node will have to be set up with the appropriate\nencapsulation and decapsulation SRv6 policies."),(0,i.kt)("p",null,"In VPP mode, Terragraph supports multiple SRv6 tunnels on a single physical\ninterface with the help of 802.1Q VLANs by creating corresponding sub-interfaces\nand adding SRv6 policies on the individual sub-interfaces. This enables\nTerragraph operators to deploy multiple SRv6 tunnels from a POP node to multiple\nCNs (for a MDU deployment) with each L2 tunnel having a unique combination of\nVLAN, source and destination IP addresses. Hence there is a requirement for all\nupstream and downstream traffic to be tagged correctly with a unique C-VLAN tag."),(0,i.kt)("p",null,"The SRv6 tunnel source and destination IPv6 addresses are formed from the node's\nown /64 address and the tunnel's VLAN ID (C-VLAN tag). An SRv6 source\n(encapsulation) IPv6 address is derived by adding the C-VLAN tag to ",(0,i.kt)("inlineCode",{parentName:"p"},":1001"),"\nalong with the node's /64 prefix. Similarly, the SRv6 destination\n(decapsulation) address uses the C-VLAN tag added and a base ",(0,i.kt)("inlineCode",{parentName:"p"},":2001")," address."),(0,i.kt)("p",null,"Since SRv6 uses native IPv6 headers and the tunnel IP addresses are derived from\nthe node /64 prefixes, Open/R automatically handles the routing for the SRv6\npackets without additional configuration."),(0,i.kt)("h4",{id:"vxlan"},"VXLAN"),(0,i.kt)("p",null,"VXLAN (Virtual Extensible LAN) is another tunnel option that is supported in VPP\nmode. VXLAN is an L2 overlay network over existing L3 networks and is\nwell-supported in both VPP and the Linux kernel. A VXLAN tunnel is formed\nbetween two endpoints (via source and destination IP addresses) and encapsulates\nan additional IPv6 + UDP + VXLAN header (56 bytes) on top of the L2 frames\nthat POP nodes and CNs receive over the wired interfaces. On POP nodes, the\nVXLAN tunnel interfaces are created in bridge mode and added to the existing POP\nbridge containing the POP interface. For CNs, the VXLAN tunnel interface is\ncross-connected to the physical interface. Currently, all VXLAN tunnels are\nplaced on the same POP bridge and VLANs will be supported in the future."),(0,i.kt)("p",null,"VXLAN tunnel redundancy is supported in VPP mode. A backup VXLAN tunnel can be\noptionally configured on CNs. A tunnel will be considered a backup tunnel if\n",(0,i.kt)("inlineCode",{parentName:"p"},"tunnelParams.primaryTunnelName")," is present, and the value should be the name of\nits corresponding primary tunnel. The primary tunnel is always the first choice\nfor carrying traffic. A CN will only switch to the backup tunnel if it detects\nthat the primary tunnel's destination node is unreachable. Once the destination\nnode of the primary tunnel is back online, the CN will switch back to the\nprimary tunnel."),(0,i.kt)("h4",{id:"l2-tunnel-configuration"},"L2 Tunnel Configuration"),(0,i.kt)("p",null,"L2 tunneling can be enabled via the ",(0,i.kt)("inlineCode",{parentName:"p"},"tunnelConfig")," node configuration field, as\nillustrated below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "tunnelConfig": {\n    "tunnel1": {\n      "enabled": true,\n      "localInterface": "TenGigabitEthernet0",\n      "dstIp": "2001::1",\n      "dstNodeName": "node-fe-19-50-01-00-8f",\n      "tunnelType": "VXLAN",\n      "tunnelParams": {\n        "vlanId": 100\n      }\n    },\n    "tunnel2": {\n      "enabled": true,\n      "localInterface": "TenGigabitEthernet0",\n      "dstIp": "3001::1",\n      "dstNodeName": "node-fe-19-22-90-10-ef",\n      "tunnelType": "VXLAN",\n      "tunnelParams": {\n        "vlanId": 100,\n        "primaryTunnelName": "tunnel1"\n      }\n    }\n  }\n}\n')),(0,i.kt)("p",null,"A ",(0,i.kt)("inlineCode",{parentName:"p"},"tunnelConfig")," in each direction (between a POP node and CN) is required for\nbi-directional traffic. While only SRv6 and VXLAN are fully supported in VPP,\nany of ",(0,i.kt)("inlineCode",{parentName:"p"},"SRV6"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"VXLAN"),", or ",(0,i.kt)("inlineCode",{parentName:"p"},"L2GRE")," can be configured. The ",(0,i.kt)("inlineCode",{parentName:"p"},"dstIp")," field is\nauto-populated by the E2E controller using the given ",(0,i.kt)("inlineCode",{parentName:"p"},"dstNodeName"),"."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"vpp_chaperone")," service uses these values to configure the L2 tunnels\nappropriately based on node type and tunnel type. For POP nodes, the POP\ninterface (",(0,i.kt)("inlineCode",{parentName:"p"},"popParams.POP_IFACE"),") should be the tunnel's ",(0,i.kt)("inlineCode",{parentName:"p"},"localInterface")," and is\nthe source interface for encapsulation. For CNs, ",(0,i.kt)("inlineCode",{parentName:"p"},"localInterface")," should be the\nCPE interface."),(0,i.kt)("h3",{id:"nat64"},"NAT64"),(0,i.kt)("p",null,"Terragraph networks require an IPv6 connection between the Terragraph cloud\nservices (e.g. E2E/NMS) and the nodes. To support IPv4 E2E/NMS, NAT64 (Network\nAddress Translation) on the POP 10G interface is the recommended solution to\nconvert control-place traffic between IPv4 and IPv6."),(0,i.kt)("p",null,"Stateful NAT64 is supported natively in VPP, and via the ",(0,i.kt)("a",{parentName:"p",href:"https://jool.mx/"},"Jool")," modules in the\nLinux kernel. With stateful NAT64, the POP node's NAT64 interface exposes only\none IPv4 address to an external IPv4 host (e.g. E2E/NMS). For each IPv4 address\nseen, it creates a flow using a unique port and performs associated\ntranslations."),(0,i.kt)("p",null,"NAT64 is intended to be deployed only for control-plane traffic, and thus will\nonly be enabled on the POP interface (",(0,i.kt)("inlineCode",{parentName:"p"},"popParams.POP_IFACE"),"). Within the IPv4\nnetwork, the IPv4 cloud services will only see a single IPv4 address\n(",(0,i.kt)("inlineCode",{parentName:"p"},"envParams.NAT64_IPV4_ADDR"),") and use that to communicate with the IPv6\nTerragraph network."),(0,i.kt)("p",null,"Terragraph uses static routing to handle IPv4 egress traffic. If E2E/NMS and\nPOP node(s) are deployed on different subnets, each POP node must enable static\nrouting (",(0,i.kt)("inlineCode",{parentName:"p"},"popParams.POP_STATIC_ROUTING"),") and specify the IPv4 address of the\nupstream router (",(0,i.kt)("inlineCode",{parentName:"p"},"popParams.GW_ADDR"),")."),(0,i.kt)("p",null,"NAT64 can be enabled via the node configuration fields shown below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "popParams": {\n    "NAT64_POP_ENABLED": "1",\n    "NAT64_IPV4_ADDR": "198.51.100.2/24",\n    "NAT64_IPV6_PREFIX": "64:ff9b::/96",\n    "GW_ADDR": "198.51.100.1",\n    "POP_STATIC_ROUTING": "1"\n  },\n  "kvstoreParams": {\n    "e2e-ctrl-url": "tcp://[64:ff9b::203.0.113.1]:7012"\n  }\n}\n')),(0,i.kt)("p",null,"External IPv4 addresses will be embedded with an IPv6 NAT64 prefix configured\nthrough ",(0,i.kt)("inlineCode",{parentName:"p"},"NAT64_IPV6_PREFIX")," (e.g. the well-known NAT64 prefix ",(0,i.kt)("inlineCode",{parentName:"p"},"64:ff9b::/96"),").\nThe resulting IPv6 prefix (e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"64:ff9b::203.0.113.1"),") will be used by the\nIPv6 Terragraph nodes to communicate with the IPv4 cloud services. These cloud\nservices (and any other external IPv4 endpoints) must be configured with the\nIPv6 representations for each IPv4 address."),(0,i.kt)("h2",{id:"policing-and-classification"},"Policing and Classification"),(0,i.kt)("p",null,"Packets that arrive on CPE interfaces with policers enabled in VPP are first\nclassified according to the DSCP field in the packet's L3 header. Inbound\ntraffic is expected to be marked with DSCP corresponding to one of the assured\nforwarding (AF) classes with low drop precedence (AFx1). Thus, the policer is\nconfigured to recognize the DSCP values corresponding to each AFx1 class. All\nother traffic will be treated by the policer as traffic class 3, green (AF11).\nThe policer will mark traffic using a two-rate/three-color policing function in\naccordance with the assured forwarding per-hop-behavior (AF PHB) defined in RFC\n2597. The standard defines 4 traffic classes and 3 colors per class. The AF DSCP\ncodes and names are as follows:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null}),(0,i.kt)("th",{parentName:"tr",align:null},"Class 1"),(0,i.kt)("th",{parentName:"tr",align:null},"Class 2"),(0,i.kt)("th",{parentName:"tr",align:null},"Class 3"),(0,i.kt)("th",{parentName:"tr",align:null},"Class 4"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Green"),(0,i.kt)("td",{parentName:"tr",align:null},"AF11 (10)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF21 (18)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF31 (26)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF41 (34)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Yellow"),(0,i.kt)("td",{parentName:"tr",align:null},"AF12 (12)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF22 (20)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF32 (28)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF42 (36)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Red"),(0,i.kt)("td",{parentName:"tr",align:null},"AF13 (14)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF23 (22)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF33 (30)"),(0,i.kt)("td",{parentName:"tr",align:null},"AF43 (38)")))),(0,i.kt)("p",null,"VPP is configured to use a two-rate/three-color policing function following RFC\n4115. A committed information rate (CIR) and excess information rate (EIR) are\ndefined for each traffic class. The peak information rate (PIR) is the sum of\nthe CIR and EIR. Committed burst size (CBS) and excess burst size (EBS) are\ncurrently not manually configurable, and are set as the CIR and EIR values over\n1 second, respectively. Traffic arriving under the CIR is marked green. Traffic\narriving over the CIR but under the PIR is marked yellow, and traffic over the\nPIR is dropped (red). When the EIR is set as 0, a one-rate/two-color policer\nwill be created instead of a two-rate/three-color policer. Traffic under the CIR\nwill be marked green, and all other traffic will be dropped. The CIR and EIR are\nset in node configuration like so:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "cpeConfig": {\n    "TenGigabitEthernet0": {\n      "policers": {\n        "0": {"cir": 1000, "eir": 2000},\n        "1": {"cir": 1000, "eir": 2000},\n        "2": {"cir": 1000, "eir": 2000},\n        "3": {"cir": 1000, "eir": 2000}\n      }\n    }\n  }\n}\n')),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"cir")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"eir")," are both in units of kilobytes per second."),(0,i.kt)("p",null,"In VPP the policers are bound to network interfaces by first defining a\nclassification table for that interface, then creating policers that match bits\nin the packet header, assigning policing sessions to the classification table,\nand finally associating the classification table with a network interface.\n",(0,i.kt)("inlineCode",{parentName:"p"},"vpp_chaperone")," performs these actions according to the node configuration\nsettings."),(0,i.kt)("p",null,"The packet classification process is illustrated below:"),(0,i.kt)("p",{align:"center"},(0,i.kt)("img",{src:"/figures/packet_classification.svg",width:"1000"})),(0,i.kt)("h2",{id:"hqos-configuration"},"HQoS Configuration"),(0,i.kt)("p",null,"Packets that reach the WiGig net interfaces are scheduled by the TG HQoS module\nin ",(0,i.kt)("a",{parentName:"p",href:"https://wiki.fd.io/view/VPP"},"VPP"),". Each DSCP value a packet may hold has a corresponding traffic class,\ncolor, and queue."),(0,i.kt)("p",null,"HQoS configuration is only supported on platforms using the VPP DPDK plugin."),(0,i.kt)("p",null,"Terragraph's HQoS hierarchy uses 16 pipes per port (1 per WiGig peer), 4 traffic\nclasses per pipe, and 1 queue per traffic class. The number of traffic classes\nand queues are determined by the constants\n",(0,i.kt)("inlineCode",{parentName:"p"},"TGHQOS_SCHED_TRAFFIC_CLASSES_PER_PIPE")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"TGHQOS_SCHED_QUEUES_PER_TRAFFIC_CLASS"),", and they can be modified to provide\nmore traffic classes or queues as necessary."),(0,i.kt)("p",null,"Compiled defaults for all configurations are found in\n",(0,i.kt)("inlineCode",{parentName:"p"},"vpp/src/plugins/dpdk/tghqos/tghqos.c"),"."),(0,i.kt)("h3",{id:"vppctl"},"vppctl"),(0,i.kt)("p",null,"The DSCP to TC/color can be inspected by executing the following vppctl command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"vpp# tghqos show interface <WiGig netif name>\n Packet IPv6 DSCP field is used for tc, color, queue:\n  IPv6 DSCP data position offset:    8\n  IPv6 DSCP data bitmask: 0x0000000000000fc0\n Stats are being cleared on read: 1\n TC translation table: ([Mapped Value Range]: tc/queue/color tc/queue/color ...)\n     [ 0 ..   7]: 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y\n     [ 8 ..  15]: 3/0/Y 3/0/Y 3/0/G 3/0/Y 3/0/Y 3/0/Y 3/0/R 3/0/Y\n     [16 ..  23]: 3/0/Y 3/0/Y 2/0/G 3/0/Y 2/0/Y 3/0/Y 2/0/R 3/0/Y\n     [24 ..  31]: 3/0/Y 3/0/Y 1/0/G 3/0/Y 1/0/Y 3/0/Y 1/0/R 3/0/Y\n     [32 ..  39]: 3/0/Y 3/0/Y 0/0/G 3/0/Y 0/0/Y 3/0/Y 0/0/R 3/0/Y\n     [40 ..  47]: 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y\n     [48 ..  55]: 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y\n     [56 ..  63]: 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y 3/0/Y\n Rate = 275000000 bytes/second\n")),(0,i.kt)("p",null,"The above TC translation table is the default table when VPP launches. The TC\ntable may be directly edited from vppctl with the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"tghqos set tctbl entry <dscp> tc <tc> queue <queue> color <color>\n")),(0,i.kt)("h3",{id:"vpp_chaperone"},"vpp_chaperone"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"vpp_chaperone")," will edit the default TC translation table on boot according to\nthe node configuration. The TC translation table can be configured with the\nfollowing schema:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "qosConfig": {\n    "dscpEntries": {\n      "0": {"tc":3, "queue":0, "color":"Y"},\n      "1": {"tc":2, "queue":0, "color":"Y"},\n      "2": {"tc":2, "queue":0, "color":"Y"},\n      "3": {"tc":1, "queue":0, "color":"G"}\n    }\n  }\n}\n')),(0,i.kt)("p",null,"Each entry in the TC table mapping is keyed on the DSCP value. Like the vppctl\ncommand, the value of each entry has three attributes, ",(0,i.kt)("inlineCode",{parentName:"p"},"tc"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"queue"),", and\n",(0,i.kt)("inlineCode",{parentName:"p"},"color"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"tc")," may be valued ","[0, 3]",", ",(0,i.kt)("inlineCode",{parentName:"p"},"queue")," may only have value 0, and ",(0,i.kt)("inlineCode",{parentName:"p"},"color"),"\nmay be ",(0,i.kt)("inlineCode",{parentName:"p"},"R"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"G"),", or ",(0,i.kt)("inlineCode",{parentName:"p"},"Y"),". Any number of DSCP entries can be specified under\n",(0,i.kt)("inlineCode",{parentName:"p"},"mapping"),". Missing DSCP entries take the default value."),(0,i.kt)("h3",{id:"weighted-round-robin-scheduling"},"Weighted Round Robin Scheduling"),(0,i.kt)("p",null,"Weighted round robin scheduling can be enabled for all interfaces using the VPP\nstartup configuration option ",(0,i.kt)("inlineCode",{parentName:"p"},"tghqos scheduling <sched_alg>")," in the VPP startup\nconfiguration file (",(0,i.kt)("inlineCode",{parentName:"p"},"/var/run/vpp/startup.conf"),"). Traffic classes are assigned\na priority and a weight which can be configured with the VPP startup\nconfiguration option ",(0,i.kt)("inlineCode",{parentName:"p"},"tghqos tc <tc> priority <priority_level> weight <weight>"),".\nExample:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"tghqos {\n  scheduling wrr\n  tc 0 priority 1 weight 16\n}\n")),(0,i.kt)("p",null,"These can also be configured for an interface with the vppctl commands,\n",(0,i.kt)("inlineCode",{parentName:"p"},"tghqos set interface scheduling <WiGig netif name> <sched_alg>")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"tghqos set interface wrr <WiGig netif name> tc <tc> priority <priority> weight <weight>"),"."),(0,i.kt)("p",null,"Configurations for each interface can be read with the command\n",(0,i.kt)("inlineCode",{parentName:"p"},"tghqos show interface <WiGig netif name>"),"."),(0,i.kt)("h2",{id:"cpe-configuration"},"CPE Configuration"),(0,i.kt)("p",null,"This section briefly describes some implementation details around CPE\nconfiguration for features which are not already covered above."),(0,i.kt)("p",null,"Terragraph nodes support one CPE interface in kernel mode and multiple CPE\ninterfaces in VPP mode. On Puma, a node can have up to 4 CPE interfaces."),(0,i.kt)("p",null,"CPE interface prefixes can be either manually assigned or automatically derived\nbased on node prefix. Stateless Address Autoconfiguration (SLAAC) and Router\nAdvertisement are configured on CPE interfaces automatically to enable prefix\nassignment for CPE devices."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In VPP mode, ",(0,i.kt)("inlineCode",{parentName:"li"},"vpp_chaperone")," performs all CPE interface configuration. DHCPv6\nprefix allocation for CPE devices is also supported through a DHCP relay via\nthe ",(0,i.kt)("inlineCode",{parentName:"li"},"cpeConfig.TenGigabitEthernetX.dhcpRelay")," node configuration field."),(0,i.kt)("li",{parentName:"ul"},"In kernel mode, the ",(0,i.kt)("inlineCode",{parentName:"li"},"squire_linux")," service configures and launches the ",(0,i.kt)("inlineCode",{parentName:"li"},"radvd"),"\nand/or ",(0,i.kt)("inlineCode",{parentName:"li"},"dhcpd")," daemons to program CPE prefixes and routes into Linux.")),(0,i.kt)("p",null,"A separate ",(0,i.kt)("inlineCode",{parentName:"p"},"cpe_operations")," program handles CPE interface events,\nadvertises/withdraws CPE prefixes from Open/R, and manages hostapd instances per\nCPE interface to provide 802.1X (see\n",(0,i.kt)("a",{parentName:"p",href:"/docs/developer/Security#security-wired-security"},"Wired Security")," for details). This program\nis invoked periodically by the ",(0,i.kt)("inlineCode",{parentName:"p"},"coop")," service."),(0,i.kt)("h2",{id:"resources"},"Resources"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Border_Gateway_Protocol"},"BGP")," - Border Gateway Protocol"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://pypi.org/project/exabgp/"},"ExaBGP")," - Python-based BGP implementation"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://frrouting.org/"},"FRRouting")," - C-based routing protocol suite"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Forwarding_information_base"},"FIB")," - Forwarding Information Base"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebook/openr"},"Open/R")," - Meta's routing platform"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://thrift.apache.org/"},"Thrift")," - Meta's interface definition language"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebook/fbthrift"},"fbthrift")," - Meta's branch of Apache Thrift"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://wiki.fd.io/view/VPP"},"VPP")," - Vector Packet Processing"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://jool.mx/"},"Jool")," - SIIT and NAT64 for Linux")))}c.isMDXComponent=!0}}]);